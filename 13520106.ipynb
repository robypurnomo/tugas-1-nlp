{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas 1 IF4072 Natural Language Processing"
      ],
      "metadata": {
        "id": "61dqmGqW_j64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deskripsi :\n",
        "\n",
        "Buatlah kode program penggunaan Spacy untuk\n",
        "\n",
        "- Sentence splitter\n",
        "- Tokenization\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "- Entity Masking\n",
        "- POS Tagger\n",
        "- Phrase Chunking\n",
        "\n",
        "\n",
        "Lengkapi kode program tersebut dengan definisi dari setiap NLP tools dalam file python-nya\n",
        "\n",
        "Dibuat dalam file .ipynb atau .py dimana setiap kode program perlu diberi komentar berupa penjelasan kode program tersebut, nama file adalah nim mahasiswa"
      ],
      "metadata": {
        "id": "ke_ziavM_npH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "ewVk2oZzByrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy-cleaner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QDbSIyFSq_d",
        "outputId": "f5c785f6-dde6-48dc-e0c0-f975542d236a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy-cleaner in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy-cleaner) (3.4.4)\n",
            "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from spacy-cleaner) (1.0.5)\n",
            "Requirement already satisfied: tqdm<4.65.0,>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from spacy-cleaner) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->spacy-cleaner) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->spacy-cleaner) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->spacy-cleaner) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_cleaner\n",
        "from spacy_cleaner.processing import removers, replacers, mutators\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYa86_nSBxhK",
        "outputId": "98197ccb-cfcb-4fd5-f59e-f937185b9dc8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text"
      ],
      "metadata": {
        "id": "nbAtnMWrCFEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = 'Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice. This is also called “language in.” Most consumers have probably interacted with NLP without realizing it. For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa. When we ask questions of these virtual assistants, NLP is what enables them to not only understand the user’s request, but to also respond in natural language. NLP applies both to written text and speech, and can be applied to all human languages. Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking. For example, some email programs can automatically suggest an appropriate reply to a message based on its content—these programs use NLP to read, analyze, and respond to your message.'\n",
        "sentence = 'ChatGPT, which stands for Chat Generative Pre-trained Transformer, is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, notable for enabling users to refine and steer a conversation towards a desired length, format, style, level of detail, and language used.'\n",
        "\n",
        "print(f'Text Paragraf : \\n{paragraph}\\n')\n",
        "print(f'Text Kalimat : \\n{sentence}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7HOQ7d-CHMa",
        "outputId": "7973d3de-3c4a-4967-a961-16119c887ca1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Paragraf : \n",
            "Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice. This is also called “language in.” Most consumers have probably interacted with NLP without realizing it. For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa. When we ask questions of these virtual assistants, NLP is what enables them to not only understand the user’s request, but to also respond in natural language. NLP applies both to written text and speech, and can be applied to all human languages. Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking. For example, some email programs can automatically suggest an appropriate reply to a message based on its content—these programs use NLP to read, analyze, and respond to your message.\n",
            "\n",
            "Text Kalimat : \n",
            "ChatGPT, which stands for Chat Generative Pre-trained Transformer, is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, notable for enabling users to refine and steer a conversation towards a desired length, format, style, level of detail, and language used.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Splitter"
      ],
      "metadata": {
        "id": "zqqL256BBba-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9LjoDUi-P4y",
        "outputId": "3c515db6-ad0e-46e1-a92c-8b71cac0747c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat ke-1 : Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language.\n",
            "Kalimat ke-2 : Natural language processing has the ability to interrogate the data with natural language text or voice.\n",
            "Kalimat ke-3 : This is also called “language in.”\n",
            "Kalimat ke-4 : Most consumers have probably interacted with NLP without realizing it.\n",
            "Kalimat ke-5 : For instance, NLP is the core technology behind virtual assistants, such as the Oracle Digital Assistant (ODA), Siri, Cortana, or Alexa.\n",
            "Kalimat ke-6 : When we ask questions of these virtual assistants, NLP is what enables them to not only understand the user’s request, but to also respond in natural language.\n",
            "Kalimat ke-7 : NLP applies both to written text and speech, and can be applied to all human languages.\n",
            "Kalimat ke-8 : Other examples of tools powered by NLP include web search, email spam filtering, automatic translation of text or speech, document summarization, sentiment analysis, and grammar/spell checking.\n",
            "Kalimat ke-9 : For example, some email programs can automatically suggest an appropriate reply to a message based on its content—these programs use NLP to read, analyze, and respond to your message.\n"
          ]
        }
      ],
      "source": [
        "def sentence_splitter(text) :\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Menyimpan split sentence\n",
        "  res = []\n",
        "  for sent in doc.sents :\n",
        "    res.append(sent)\n",
        "  return res\n",
        "\n",
        "result = sentence_splitter(paragraph)\n",
        "i = 1\n",
        "for sent in result:\n",
        "  print(f'Kalimat ke-{i} : {sent}')\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "wS7w9UIHBfnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text) :\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Menyimpan tokenize word\n",
        "  res = []\n",
        "  for token in doc:\n",
        "    res.append(token)\n",
        "  return res\n",
        "\n",
        "result = tokenize(sentence)\n",
        "for word in result :\n",
        "  print(word.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTEzAQDrBiRj",
        "outputId": "3406136e-9516-483f-ee7f-b14019d416d3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT\n",
            ",\n",
            "which\n",
            "stands\n",
            "for\n",
            "Chat\n",
            "Generative\n",
            "Pre\n",
            "-\n",
            "trained\n",
            "Transformer\n",
            ",\n",
            "is\n",
            "a\n",
            "large\n",
            "language\n",
            "model\n",
            "-\n",
            "based\n",
            "chatbot\n",
            "developed\n",
            "by\n",
            "OpenAI\n",
            "and\n",
            "launched\n",
            "on\n",
            "November\n",
            "30\n",
            ",\n",
            "2022\n",
            ",\n",
            "notable\n",
            "for\n",
            "enabling\n",
            "users\n",
            "to\n",
            "refine\n",
            "and\n",
            "steer\n",
            "a\n",
            "conversation\n",
            "towards\n",
            "a\n",
            "desired\n",
            "length\n",
            ",\n",
            "format\n",
            ",\n",
            "style\n",
            ",\n",
            "level\n",
            "of\n",
            "detail\n",
            ",\n",
            "and\n",
            "language\n",
            "used\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove punctuation"
      ],
      "metadata": {
        "id": "ZLdP5yeoIZih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(text) :\n",
        "  model = spacy.load(\"en_core_web_sm\")\n",
        "  tes = spacy_cleaner.Cleaner(model, True, True)\n",
        "  return tes.clean(text)"
      ],
      "metadata": {
        "id": "9Y6qOCo8IdWH"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "spaCy tidak menyediakan function untuk melakukan stemming"
      ],
      "metadata": {
        "id": "lgmpoid3E3OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def stemming(text) :\n",
        "  # remove puncuation dan stop-words\n",
        "  clean = cleaner([text])[0]\n",
        "  words = tokenize(clean)\n",
        "\n",
        "  # melakukan stemming dengan nltk\n",
        "  print('Hasil Stemming :\\n')\n",
        "  ps = PorterStemmer()\n",
        "  for w in words:\n",
        "    print(w.text, \" --> \", ps.stem(w.text))\n",
        "\n",
        "stemming(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JpDNee6E5OX",
        "outputId": "7db3e071-645c-4355-beab-104428015c85"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning Progress: 100%|██████████| 1/1 [00:00<00:00, 46.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Stemming :\n",
            "\n",
            "chatgpt  -->  chatgpt\n",
            "stands  -->  stand\n",
            "chat  -->  chat\n",
            "generative  -->  gener\n",
            "pre  -->  pre\n",
            "trained  -->  train\n",
            "transformer  -->  transform\n",
            "large  -->  larg\n",
            "language  -->  languag\n",
            "model  -->  model\n",
            "based  -->  base\n",
            "chatbot  -->  chatbot\n",
            "developed  -->  develop\n",
            "openai  -->  openai\n",
            "launched  -->  launch\n",
            "november  -->  novemb\n",
            "notable  -->  notabl\n",
            "enabling  -->  enabl\n",
            "users  -->  user\n",
            "refine  -->  refin\n",
            "steer  -->  steer\n",
            "conversation  -->  convers\n",
            "desired  -->  desir\n",
            "length  -->  length\n",
            "format  -->  format\n",
            "style  -->  style\n",
            "level  -->  level\n",
            "detail  -->  detail\n",
            "language  -->  languag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "kSs84rjSL87T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(text) :\n",
        "  # remove puncuation dan stop-words\n",
        "  clean = cleaner([text])[0]\n",
        "\n",
        "  # melakukan lemmatization dengan spaCy\n",
        "  print('Hasil Lemmatization :\\n')\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc = nlp(clean)\n",
        "  for token in doc:\n",
        "      print(token.text, \"-->\", token.lemma_)\n",
        "\n",
        "lemmatization(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeioHJRpMAoK",
        "outputId": "8868b92d-9e1c-49e0-897f-677ae8b2febe"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning Progress: 100%|██████████| 1/1 [00:00<00:00, 47.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Lemmatization :\n",
            "\n",
            "chatgpt --> chatgpt\n",
            "stands --> stand\n",
            "chat --> chat\n",
            "generative --> generative\n",
            "pre --> pre\n",
            "trained --> train\n",
            "transformer --> transformer\n",
            "large --> large\n",
            "language --> language\n",
            "model --> model\n",
            "based --> base\n",
            "chatbot --> chatbot\n",
            "developed --> develop\n",
            "openai --> openai\n",
            "launched --> launch\n",
            "november --> november\n",
            "notable --> notable\n",
            "enabling --> enable\n",
            "users --> user\n",
            "refine --> refine\n",
            "steer --> steer\n",
            "conversation --> conversation\n",
            "desired --> desire\n",
            "length --> length\n",
            "format --> format\n",
            "style --> style\n",
            "level --> level\n",
            "detail --> detail\n",
            "language --> language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity Masking"
      ],
      "metadata": {
        "id": "QUzHoshUMywS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_masking(text) :\n",
        "  # remove puncuation dan stop-words\n",
        "  clean = cleaner([text])[0]\n",
        "\n",
        "  # entity masking (named entity recognition)\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc = nlp(clean)\n",
        "  for w in doc:\n",
        "    print((w, w.ent_iob_, w.ent_type_))\n",
        "\n",
        "entity_masking(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LDssNH9M16h",
        "outputId": "591b3ba7-daf8-4033-b68c-73990ade8805"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning Progress: 100%|██████████| 1/1 [00:00<00:00, 44.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(chatgpt, 'O', '')\n",
            "(stands, 'O', '')\n",
            "(chat, 'O', '')\n",
            "(generative, 'O', '')\n",
            "(pre, 'O', '')\n",
            "(trained, 'O', '')\n",
            "(transformer, 'O', '')\n",
            "(large, 'O', '')\n",
            "(language, 'O', '')\n",
            "(model, 'O', '')\n",
            "(based, 'O', '')\n",
            "(chatbot, 'O', '')\n",
            "(developed, 'O', '')\n",
            "(openai, 'O', '')\n",
            "(launched, 'O', '')\n",
            "(november, 'B', 'DATE')\n",
            "(notable, 'O', '')\n",
            "(enabling, 'O', '')\n",
            "(users, 'O', '')\n",
            "(refine, 'O', '')\n",
            "(steer, 'O', '')\n",
            "(conversation, 'O', '')\n",
            "(desired, 'O', '')\n",
            "(length, 'O', '')\n",
            "(format, 'O', '')\n",
            "(style, 'O', '')\n",
            "(level, 'O', '')\n",
            "(detail, 'O', '')\n",
            "(language, 'O', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagger"
      ],
      "metadata": {
        "id": "Ik97KrbGOmNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagger(text) :\n",
        "  # POS Tagger\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "    print(word, ':', word.pos_) # Mengambil word.pos_ untuk melihat POS tagger dari kata/token tsb\n",
        "\n",
        "pos_tagger(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etA7tYGNOprW",
        "outputId": "7a0c1274-8564-4876-b7f1-d42e8779cc15"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT : PROPN\n",
            ", : PUNCT\n",
            "which : PRON\n",
            "stands : VERB\n",
            "for : ADP\n",
            "Chat : PROPN\n",
            "Generative : PROPN\n",
            "Pre : PROPN\n",
            "- : PUNCT\n",
            "trained : VERB\n",
            "Transformer : PROPN\n",
            ", : PUNCT\n",
            "is : AUX\n",
            "a : DET\n",
            "large : ADJ\n",
            "language : NOUN\n",
            "model : NOUN\n",
            "- : PUNCT\n",
            "based : VERB\n",
            "chatbot : NOUN\n",
            "developed : VERB\n",
            "by : ADP\n",
            "OpenAI : PROPN\n",
            "and : CCONJ\n",
            "launched : VERB\n",
            "on : ADP\n",
            "November : PROPN\n",
            "30 : NUM\n",
            ", : PUNCT\n",
            "2022 : NUM\n",
            ", : PUNCT\n",
            "notable : ADJ\n",
            "for : ADP\n",
            "enabling : VERB\n",
            "users : NOUN\n",
            "to : PART\n",
            "refine : VERB\n",
            "and : CCONJ\n",
            "steer : VERB\n",
            "a : DET\n",
            "conversation : NOUN\n",
            "towards : ADP\n",
            "a : DET\n",
            "desired : VERB\n",
            "length : NOUN\n",
            ", : PUNCT\n",
            "format : NOUN\n",
            ", : PUNCT\n",
            "style : NOUN\n",
            ", : PUNCT\n",
            "level : NOUN\n",
            "of : ADP\n",
            "detail : NOUN\n",
            ", : PUNCT\n",
            "and : CCONJ\n",
            "language : NOUN\n",
            "used : VERB\n",
            ". : PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Chunking"
      ],
      "metadata": {
        "id": "E_xwzWoVQLya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_chunking(text) :\n",
        "  # Phrase Noun Chunking\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "  for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n",
        "\n",
        "phrase_chunking(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUngzmxbQR0c",
        "outputId": "ce1d6aac-a9f2-46dc-dc46-175a73e21efa"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT\n",
            "which\n",
            "Chat Generative Pre-trained Transformer\n",
            "a large language model-based chatbot\n",
            "OpenAI\n",
            "November\n",
            "users\n",
            "a conversation\n",
            "a desired length, format, style, level\n",
            "detail\n",
            "language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan\n",
        "\n",
        "Sebenarnya dengan menggunakan library dari spaCy ini yang sangat simple dengan hanya memanggil ``` spacy.load(\"en_core_web_sm\")(text) ```, spaCy sudah menyediakan berbagai dasar operasi NLP. Selanjutnya kita hanya perlu memanggilnya saja sesuai yang kita perlukan."
      ],
      "metadata": {
        "id": "sOQwfpZXRoZw"
      }
    }
  ]
}